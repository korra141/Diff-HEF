# -*- coding: utf-8 -*-
"""MLP Measurement Model with Analytical HEF PM on S1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nm4bRj9HW6t4RraArnOC6adQ_3R3eOjp
"""

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.optim as optim
import torch.fft
import torch.nn as nn
import time
import math
import os
import wandb
import sys
import pdb
from torchinterp1d import interp1d

base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(base_path)

from src.utils.visualisation import plot_s1_energy, plotting_von_mises, generate_gif


def generating_data_S1(batch_size, n_samples, trajectory_length, measurement_noise, recreate=False,step=0.1, shuffle_flag=True):
    """Generates training data for a system with circular motion.

    Args:
      batch_size: The number of samples in each batch.
      n_samples: The total number of samples to generate.
      trajectory_length: The length of each trajectory.
      measurement_noise: The standard deviation of the measurement noise.
      step: The step between poses in trajectory
      shuffle_flag: Whether to shuffle the samples,

    Returns:
      Flattened pose and noisy measurement data in a TensorDataset.
    """
    data_path = os.path.join(base_path, 'data', f's1_simple_dataset_{measurement_noise}.pt')
    if recreate or not os.path.exists(data_path):
        print('Generating Data and Saving')
        starting_positions = np.linspace(0, 2 * np.pi, n_samples, endpoint=False)
        true_trajectories = np.ndarray((n_samples, trajectory_length))
        measurements = np.ndarray((n_samples, trajectory_length))

        for i in range(n_samples):
            # Generate a circular trajectory with a random starting position.
            initial_angle = starting_positions[i]
            trajectory = initial_angle + np.arange(trajectory_length) * step
            true_trajectories[i] = trajectory

            # Add Gaussian noise to the measurements.
            measurements[i] = (trajectory + np.random.normal(0, measurement_noise, trajectory_length)) % (2 * np.pi)

        measurements_ = torch.from_numpy(measurements % (2 * np.pi))[:, :, None].type(torch.FloatTensor)
        ground_truth_ = torch.from_numpy(true_trajectories % (2 * np.pi))[:, :, None].type(torch.FloatTensor)
        # ground_truth_flatten = torch.flatten(ground_truth_)[:, None].type(torch.FloatTensor)
        # measurements_flatten = torch.flatten(measurements_)[:, None].type(torch.FloatTensor)
        train_dataset = torch.utils.data.TensorDataset(ground_truth_, measurements_)
        torch.save(train_dataset, data_path)
    else:
        print('Loading Data')
        train_dataset = torch.load(data_path)

    # Split the dataset into training and validation sets
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=shuffle_flag)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, drop_last=True, shuffle=False)
    
    return train_loader, val_loader
    


def total_loss(posterior, measurement_likelihood, value, band_limit, lambda_w):
    posterior_nll = loss_fn(posterior, value, band_limit)
    measurement_likelihood_nll = loss_fn(measurement_likelihood, value, band_limit)
    return lambda_w * posterior_nll + (1 - lambda_w) * measurement_likelihood_nll


def loss_fn(energy, value, grid_size, print_=False):
    """Computes the loss function for the circular motion model.

    This function calculates the loss based on the negative log likelihood of the predicted distribution with the ground truth noisy measurements value
    Args:
      mu: The mean angle of the predicted distribution.
      cov: The covariance of the predicted distribution.
      measurements: The observed measurements.
      grid_size: The number of grid points to use for calculating the energy.

    Returns:
      ln_z_: The computed loss value.

    """

    eta = torch.fft.fftshift(torch.fft.fft(energy, dim=-1), dim=-1)
    maximum = torch.max(energy, dim=-1).values.unsqueeze(-1)
    moments = torch.fft.fft(torch.exp(energy - maximum), dim=-1)
    ln_z_ = torch.log(2 * math.pi * moments[..., 0] / grid_size).real.unsqueeze(-1) + maximum

    # taking inverse FFT over a set of frequencies ranging
    k_values = torch.arange(grid_size) - grid_size / 2
    k_values = k_values.unsqueeze(-1).unsqueeze(0)  # [ 1, num_samples, 1]

    # print(k_values.shape)
    value = value.unsqueeze(-1)  # [batch_size, 1,1]
    # print(value.shape)
    exponential_term = torch.exp(1j * k_values * value)  # [batch_size, num_samples, 1]
    # print(exponential_term.size())
    inverse_transform = (eta.unsqueeze(-1) * exponential_term).sum(dim=1).real  # [batch_size, 1]
    if print_:
        print(torch.mean(inverse_transform/ grid_size))
    # print(inverse_transform.shape)
    return torch.mean(-inverse_transform / grid_size + ln_z_)


def unnormalized_von_mises_log_prob(samples, mu, kappa):
    """
    Computes the unnormalized log probability of a von Mises distribution.

    Args:
        samples: The input angle (in radians).  Shape: [batch_size, 1]
        mu: The mean angle of the von Mises distribution (in radians). Shape: [batch_size, 1]
        kappa: The concentration parameter of the von Mises distribution. Shape: [batch_size, 1]

    Returns:
        The unnormalized log probability of the von Mises distribution.
    """

    # Ensure that all inputs are PyTorch tensors.
    if not isinstance(samples, torch.Tensor):
        samples = torch.tensor(samples, dtype=torch.float32)
    if not isinstance(mu, torch.Tensor):
        mu = torch.tensor(mu, dtype=torch.float32)
    if not isinstance(kappa, torch.Tensor):
        kappa = torch.tensor(kappa, dtype=torch.float32)

    # Compute the unnormalized log probability.
    log_prob = kappa * torch.cos(samples - mu)

    return log_prob


def analytical_energy(mu, cov, band_limit):
    batch_size = mu.size(0)
    samples = torch.tile(torch.linspace(0.0, 2 * torch.pi, band_limit + 1)[:-1].unsqueeze(0),(batch_size, 1))
    kappa = np.reciprocal(cov)
    process_energy = unnormalized_von_mises_log_prob(samples, mu, kappa)
    return process_energy


class MeasurementModel(nn.Module):
    def __init__(self, input_size1, input_size2, hidden_size, output_size):
        super(MeasurementModel, self).__init__()
        self.fc1_1 = nn.Linear(input_size1, hidden_size)
        self.fc1_2 = nn.Linear(input_size2, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(2 * hidden_size, output_size)
        self.tanh = nn.Tanh()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x1, x2):
        out1 = self.fc1_1(x1)
        out2 = self.fc1_2(x2)
        out = self.sigmoid(torch.cat((out1, out2), dim=1))
        out = self.fc2(out)
        return out + x1
    
class EnergyNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(EnergyNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)


class HEFilter(nn.Module):
    def __init__(self, band_limit,step_size=0.1):
        super(HEFilter, self).__init__()
        self.band_limit = band_limit
        self.step_size = step_size

    def predict(self, prior, process):
        ln_z_1 = self.normalisation_constant(prior)
        ln_z_2 = self.normalisation_constant(process)
        prob_1 = torch.exp(prior - ln_z_1)
        prob_2 = torch.exp(process - ln_z_2)

        moments_1 = torch.fft.fft(prob_1, dim=-1)
        moments_2 = torch.fft.fft(prob_2, dim=-1)
        moments_convolve = moments_1 * moments_2

        eta, energy = self.convert_moments_eta_energy(moments_convolve)
        return eta, energy

    def convert_moments_eta_energy(self, moments):
        prob = torch.fft.ifft(moments, dim=-1)
        ln_z_ = torch.unsqueeze(torch.real(torch.log(2 * math.pi * moments[:, 0] / self.band_limit)), 1)
        prob_real = torch.real(prob)
        prob_process = torch.where(prob_real > 0, prob_real, 1e-8)
        energy = torch.log(prob_process)
        eta = torch.fft.fft(energy, dim=-1)
        return eta, energy

    def convert_from_energy_eta(self, energy_samples):
        ln_z_ = self.normalisation_constant(energy_samples)
        eta = torch.fft.fft(energy_samples - ln_z_)
        return eta

    def convert_from_eta_energy(self, eta):
        energy = torch.fft.ifft(eta)
        prob_unnorm = torch.exp(energy)
        moments = torch.fft.fft(prob_unnorm)
        ln_z_ = torch.unsqueeze(torch.real(torch.log(2 * math.pi * moments[:, 0] / self.band_limit)), 1)
        energy_norm = torch.real(energy) - ln_z_
        return energy_norm

    def update(self, predict, measurements):
        eta = predict + self.convert_from_energy_eta(measurements)
        return self.convert_from_eta_energy(eta)

    def normalisation_constant(self, energy):
        maximum = torch.max(energy, dim=-1).values.unsqueeze(-1)
        moments = torch.fft.fft(torch.exp(energy - maximum))
        ln_z_ = torch.unsqueeze(torch.real(torch.log(2 * math.pi * moments[:, 0] / self.band_limit)), 1) + maximum
        return ln_z_
    
    def convert_energy_density(self,energy):
        ln_z_ = self.normalisation_constant(energy)
        energy_norm = torch.real(energy) - ln_z_
        prob_norm = torch.exp(energy_norm)
        return prob_norm
    
    def mode(self, energy ,ground_truth=None, n_modes=None):
        # Step 1: Get the maximum values and their indices along the last dimension (dim=-1)
          # max over columns, shape (batch_size,)
        predicted_density = self.convert_energy_density(energy)
        if n_modes is not None:
            max_vals, max_indices = torch.topk(predicted_density, n_modes, dim=-1)
        else:
            max_vals, max_indices = torch.max(predicted_density, dim=-1)
        # Step 2: Get the indices of the maximum values along the last dimension (dim=-1)
        # if self.local_grid:
        #     poses_mode = (self.range_theta * max_indices / self.grid_size) + ground_truth  - self.range_theta/2
        # else:
        poses_mode = (2*math.pi  * max_indices / self.band_limit)
        # poses_mode = (max_indices * self.step_size)

        return poses_mode.unsqueeze(-1)

    def mean(self,energy):
        # batch_size = predicted_density.shape[0]
        grid = torch.linspace(0, 2*math.pi, self.band_limit+1)[:-1]
        diff = torch.ones_like(grid) * (2 * math.pi / self.band_limit)
        density = self.convert_energy_density(energy)
        mean = torch.sum(density* grid * diff, dim=-1)
        return mean.unsqueeze(-1)


# def train(args, measurement_model,train_loader, folder_path):
#     training_path = os.path.join(folder_path, "training")
#     os.makedirs(training_path, exist_ok=True)
#     optimizer = optim.Adam(measurement_model.parameters(), lr=args.learning_rate)
#     hef = HEFilter(args.band_limit)
#     for epoch in range(args.num_epochs):
#         print(f"Training at epoch {epoch}")
#         loss_tot = 0
#         nll_posterior_tot = 0
#         nll_likelihood_tot = 0
#         # epoch_folder_path = os.path.join(folder_path, f"epoch_{epoch}")
#         for i, (measurements, ground_truth) in enumerate(train_loader):
#             prior = analytical_energy(ground_truth[:, 0], args.initial_cov, args.band_limit)
#             control = torch.ones((args.batch_size, 1)) * args.step_size + torch.normal(0, args.noise_p,
#                                                                              size=(args.batch_size, 1))  # Fixed Control
#             for j in range(args.traj_len):
#                 energy_true_z = analytical_energy(measurements[:, j], args.input_cov_z, args.band_limit)
#                 energy_process = analytical_energy(control, args.noise_p ** 2, args.band_limit)

#                 # Predict Step
#                 eta_bel_x_t_bar, energy_bel_x_t_bar = hef.predict(prior, energy_process)
#                 mean_energy_bel_x_t_bar = hef.mean_hef(energy_bel_x_t_bar)

#                 # P(z_t | x_t) Measurement Step
#                 energy_pred_z = measurement_model(mean_energy_bel_x_t_bar)

#                 # Calculate Posterior
#                 with torch.no_grad():
#                     energy_posterior = hef.update(eta_bel_x_t_bar, energy_pred_z)

#                 # Current posterior is prior for the next step (j + 1)
#                 prior = energy_posterior

#                 # Calculate loss
#                 # loss = total_loss(energy_posterior, energy_pred_z, ground_truth[:, j], args.band_limit, args.current_lambda)
#                 loss = loss_fn(energy_pred_z, ground_truth[:, j], args.band_limit)
#                 nll_posterior = loss_fn(energy_posterior, ground_truth[:, j], args.band_limit)

#                 # Update Measurement Model
#                 optimizer.zero_grad()
#                 loss.backward()
#                 optimizer.step()

#                 loss_tot += loss.item()
#                 nll_posterior_tot += nll_posterior.item()
#                 # nll_likelihood_tot += nll_likelihood.item()

#                 if epoch % 10 == 0 and i == 0:
#                     # os.makedirs(epoch_folder_path, exist_ok=True)
#                     if j % 5 == 0:
#                         plot_distributions(energy_posterior, energy_pred_z, energy_bel_x_t_bar, ground_truth,
#                                            measurements, 0, j, epoch, "training_hef", training_path)
#         print(f"Epoch {epoch} Loss: {loss_tot / (len(train_loader)* args.traj_len)} NLL Posterior: {nll_posterior_tot / (len(train_loader) * args.traj_len)}")
#         model_save_path = os.path.join(training_path, 'model.pth')
#         torch.save(measurement_model.state_dict(), model_save_path)
#         print(f"Model saved to {model_save_path}")

#         return measurement_model



def main(args):
    # measurement_model = MeasurementModel(band_limit, band_limit, hidden_size, band_limit)
    measurement_model = EnergyNetwork(1, 10, args.band_limit)
    train_loader,test_loader = generating_data_S1(args.batch_size, args.n_trajs, args.traj_len, args.noise_z, False, args.step_size, True)
    if args.model_path is not None and os.path.exists(args.model_path):
        print('Loading pre-trained model')
        measurement_model.load_state_dict(torch.load(args.model_path))
    else:
        print('Initializing new model')
        measurement_model.apply(init_weights)
        # measurement_model = train(args, measurement_model,train_loader, folder_path)
        
    hef = HEFilter(args.band_limit, args.step_size)
    c_time = time.strftime("%Y_%m_%d_%H_%M_%S")
    folder_path = f"{base_path}/logs/s1_filter/{c_time}"
    os.makedirs(folder_path, exist_ok=True)
    print(f"Saving logs to {folder_path}")

    nll_posterior_tot = 0
    rmse_tot = 0
    ground_truth, measurements = next(iter(test_loader))

    prior = analytical_energy(ground_truth[:, 0], args.initial_cov, args.band_limit)
    prior_hef = analytical_energy(ground_truth[:, 0], args.initial_cov, args.band_limit)
    prior_true = analytical_energy(ground_truth[:, 0], args.initial_cov, args.band_limit)
    control = torch.ones((args.batch_size, 1)) * args.step_size + torch.normal(0, args.noise_p, size=(args.batch_size, 1))  # Fixed Control
    
    nll_posterior_tot = 0
    rmse_tot = 0
    nll_posterior_tot_hef = 0
    rmse_tot_hef = 0
    nll_posterior_tot_true = 0
    rmse_tot_true = 0
    nll_posterior_check_tot = 0
    nll_posterior_true_check_tot = 0
    nll_posterior_hef_check_tot = 0
    for j in range(args.traj_len):
        grid = torch.linspace(0, 2*math.pi, args.band_limit+1)[:-1]
        # pdb.set_trace()
        # Inference over differentiable HEF
        energy_process = analytical_energy(control, args.noise_p ** 2, args.band_limit)
        # Predict Step
        eta_bel_x_t_bar, energy_bel_x_t_bar = hef.predict(prior, energy_process)
        # mean_bel_x_t_bar = hef.mode(energy_bel_x_t_bar)
        # P(z_t | x_t) Measurement Step
        # energy_pred_z = measurement_model(energy_true_z, energy_bel_x_t_bar)
        # noisy_measurement = measurements[:, j] + torch.normal(0, np.sqrt(args.input_cov_z), size=(args.batch_size, 1))
        energy_pred_z = measurement_model(measurements[:, j])
        # energy_pred_z = measurement_model(ground_truth[:, j])
        energy_posterior = hef.update(eta_bel_x_t_bar, energy_pred_z)
        # Current posterior is prior for the next step (j + 1)
        prior = energy_posterior
        nll_posterior = loss_fn(energy_posterior, ground_truth[:, j], args.band_limit)
        nll_posterior_check = torch.mean(-interp1d(grid,energy_posterior,ground_truth[:, j]) + hef.normalisation_constant(energy_posterior))
  


        # Analytical HEF
        energy_z_hef = analytical_energy(measurements[:, j], args.input_cov_z, args.band_limit)
        eta_bel_x_t_bar_hef, energy_bel_x_t_bar_hef = hef.predict(prior_hef, energy_process)
        energy_posterior_hef = hef.update(eta_bel_x_t_bar_hef, energy_z_hef)
        prior_hef = energy_posterior_hef
        nll_posterior_hef = loss_fn(energy_posterior_hef, ground_truth[:, j], args.band_limit)
        nll_posterior_hef_check = torch.mean(-interp1d(grid,energy_posterior_hef,ground_truth[:, j]) + hef.normalisation_constant(energy_posterior_hef))

        # True Posterior 
        energy_z_true = analytical_energy(measurements[:, j], args.noise_z **2 , args.band_limit)
        eta_bel_x_t_bar_true, energy_bel_x_t_bar_true = hef.predict(prior_true, energy_process)
        energy_posterior_true = hef.update(eta_bel_x_t_bar_true, energy_z_true)
        prior_true = energy_posterior_true
        nll_posterior_true = loss_fn(energy_posterior_true, ground_truth[:, j], args.band_limit)
     
        nll_posterior_true_check = torch.mean(-interp1d(grid,energy_posterior_true,ground_truth[:, j]) + hef.normalisation_constant(energy_posterior_true))
        # Gaussian Posterior EKF 

        # Pretrained LSTM Model


        # Metrics
        nll_posterior_iter = nll_posterior.item()
        nll_posterior_tot += nll_posterior_iter

        nll_posterior_check_tot += nll_posterior_check.item()

        mean_posterior = hef.mean(energy_posterior)
        rmse = torch.sqrt(torch.mean((ground_truth[:, j] - mean_posterior) ** 2))
        rmse_tot += rmse

        nll_posterior_iter_hef = nll_posterior_hef.item()
        nll_posterior_tot_hef += nll_posterior_iter_hef

        nll_posterior_hef_check_tot += nll_posterior_hef_check.item()

        mean_posterior_hef = hef.mean(energy_posterior_hef)
        rmse_hef = torch.sqrt(torch.mean((ground_truth[:, j] - mean_posterior_hef) ** 2))
        rmse_tot_hef += rmse_hef
                                   
        nll_posterior_iter_true = nll_posterior_true.item()
        nll_posterior_tot_true += nll_posterior_iter_true

        nll_posterior_true_check_tot = nll_posterior_true_check.item()
        mean_posterior_true = hef.mean(energy_posterior_true)
        rmse_true = torch.sqrt(torch.mean((ground_truth[:, j] - mean_posterior_true) ** 2))
        rmse_tot_true += rmse_true

        print(f"Iteration {j} Diff HEF NLL Posterior: {nll_posterior_iter}  RMSE: {rmse}")
        print(f"Iteration {j} Analytical HEF NLL Posterior: {nll_posterior_iter_hef}  RMSE: {rmse_hef}")
        print(f"Iteration {j} True Posterior NLL Posterior: {nll_posterior_iter_true}  RMSE: {rmse_true}")
        print(f"Interpolator Check: DiffHEF {nll_posterior_check.item()} HEF {nll_posterior_hef_check} True Posterior {nll_posterior_true_check.item()}")
        wandb.log({"Iteration": j, "NLL Posterior Diff HEF": nll_posterior_iter, "RMSE Diff HEF": rmse
                    , "NLL Posterior Analytical HEF": nll_posterior_iter_hef, "RMSE Analytical HEF": rmse_hef
                    , "NLL Posterior True Posterior": nll_posterior_iter_true, "RMSE True Posterior": rmse_true})
        if j % 6 == 0:
            # fig, ax = plt.subplots()
            # ax.plot(torch.cos(mean_bel_x_t_bar[0, 0]), torch.sin(mean_bel_x_t_bar[0, 0]), 'o', label="mean pred z")
            plot_distributions(energy_posterior, energy_pred_z, energy_bel_x_t_bar, ground_truth,
                                measurements, 0, j, 0, "diff_hef", folder_path)
            plot_distributions(energy_posterior_hef, energy_z_hef, energy_bel_x_t_bar_hef, ground_truth,
                                measurements, 0, j, 0, "hef", folder_path)
            plot_distributions(energy_posterior_true, energy_z_true, energy_bel_x_t_bar_true, ground_truth,
                                measurements, 0, j, 0, "true", folder_path)
            
                
    average_nll_traj = nll_posterior_tot/args.traj_len
    average_rmse = rmse_tot/args.traj_len

    average_nll_traj_hef = nll_posterior_tot_hef/args.traj_len
    average_rmse_hef = rmse_tot_hef/args.traj_len

    average_nll_traj_true = nll_posterior_tot_true/args.traj_len
    average_rmse_true = rmse_tot_true/args.traj_len

    average_nll_traj_check = nll_posterior_check_tot/args.traj_len
    average_nll_traj_hef_check = nll_posterior_hef_check_tot/args.traj_len
    average_nll_traj_true_check = nll_posterior_true_check_tot/args.traj_len

    print(f"Avg NLL Posterior Diff HEF: {average_nll_traj} Average RMSE: {average_rmse}")
    print(f"Avg NLL Posterior Analytical HEF: {average_nll_traj_hef}  Average RMSE: {average_rmse_hef}")
    print(f"Avg NLL Posterior True Posterior: {average_nll_traj_true} Average RMSE: {average_rmse_true}")
    print(f"Avg Interpolator: DiffHEF {average_nll_traj_check} HEF {average_nll_traj_hef_check} True {average_nll_traj_true_check}")

    wandb.log({"Average NLL Posterior Diff HEF": average_nll_traj, "Average RMSE Diff HEF": average_rmse, 
               "Average NLL Posterior Analytical HEF": average_nll_traj_hef, "Average RMSE Analytical HEF": average_rmse_hef,
               "Average NLL Posterior True Posterior": average_nll_traj_true, "Average RMSE True Posterior": average_rmse_true})
    if os.path.exists(folder_path):
        print(f"Generating gif for diff hef")
        video_path = f"{folder_path}/diff_hef.gif"
        generate_gif(folder_path, video_path, prefix="diff_hef")
        wandb.log({rf"diff_hef": wandb.Video(video_path, format="gif", )})



def plot_distributions(energy_posterior, energy_pred_z, energy_bel_x_t_bar, ground_truth, measurements, i, j, epoch, title, folder_path, ax=None):
    print(f"Plotting Epoch {epoch} Traj{i} Iteration {j}")
    if ax is None:
        fig, ax = plt.subplots()
    energy_posterior = energy_posterior[i].detach()
    energy_pred_z = energy_pred_z[i].detach()
    energy_bel_x_t_bar = energy_bel_x_t_bar[i].detach()

    ax = plot_s1_energy([energy_posterior, energy_pred_z, energy_bel_x_t_bar],
                        ["posterior distribution", "measurement distribution", "predict distribution"], ax)
    ax.plot(torch.cos(measurements[i, j]), torch.sin(measurements[i, j]), 'o', label="measurement data")
    ax.plot(torch.cos(ground_truth[i, j]), torch.sin(ground_truth[i, j]), 'o', label="pose data")
    ax.set_title(f"Epoch {epoch} Traj{i} Iteration {j}", loc='center')
    ax.legend(bbox_to_anchor=(0.85, 1), loc='upper left', fontsize='x-small')
    plt.savefig(f"{folder_path}/{title}_iter_{j}.png", format='png', dpi=300)
    plt.close()
    return ax


import argparse


def parse_args():
    parser = argparse.ArgumentParser(description='Train MLP Measurement Model with Analytical HEF PM on S1')
    parser.add_argument('--batch_size', type=int, default=45, help='Batch size for training')
    parser.add_argument('--n_trajs', type=int, default=300, help='Number of trajectories')
    parser.add_argument('--traj_len', type=int, default=30, help='Length of each trajectory')
    parser.add_argument('--noise_z', type=float, default=0.2, help='Measurement noise (Gaussian)')
    parser.add_argument('--noise_p', type=float, default=0.2, help='Process noise')
    parser.add_argument('--initial_cov', type=float, default=0.01, help='Initial covariance')
    parser.add_argument('--band_limit', type=int, default=30, help='Band limit')
    parser.add_argument('--step_size', type=float, default=0.1, help='Step size')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate')
    parser.add_argument('--num_epochs', type=int, default=1000, help='Number of epochs')
    parser.add_argument('--input_cov_z', type=float, default=0.01, help='Input covariance for Z')
    parser.add_argument('--initial_lambda', type=float, default=0.7, help='Initial lambda for decay loss')
    parser.add_argument('--decay_rate', type=float, default=0.5, help='Decay rate for lambda decay loss')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--decay_lambda', type=int, choices=[0, 1], default=0,
                        help='flag set to true do lambda decay for the loss')
    parser.add_argument('--true_cov', type=float, default=0.04, help='True covariance for Z')
    parser.add_argument('--model_path', type=str, default=None, help='Path to the pre-trained model')
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    torch.manual_seed(args.seed)

    # If you are using CUDA
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)

    wandb.init(project="Diff-HEF", entity="korra141",
               tags=["s1filter", "analytical_PM", "pretrained_MM", "overconfident MM HEF", "uncerconfident PM HEF"],
               name="s1_filter_with_learning_MM",
               config=args)
    main(args)