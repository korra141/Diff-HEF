# -*- coding: utf-8 -*-
"""MLP Measurement Model with Analytical HEF PM on S1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nm4bRj9HW6t4RraArnOC6adQ_3R3eOjp
"""

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.optim as optim
import torch.fft
import torch.nn as nn
import time
import math
import os
import wandb
import sys

base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(base_path)

from utils.visualisation import plot_s1_energy, plotting_von_mises, generate_gif


def generating_data_S1(batch_size, n_samples, trajectory_length, measurement_noise, step=0.1, shuffle_flag=True):
    """Generates training data for a system with circular motion.

    Args:
      batch_size: The number of samples in each batch.
      n_samples: The total number of samples to generate.
      trajectory_length: The length of each trajectory.
      measurement_noise: The standard deviation of the measurement noise.
      step: The step between poses in trajectory
      shuffle_flag: Whether to shuffle the samples,

    Returns:
      Flattened pose and noisy measurement data in a TensorDataset.
    """
    data_path = os.path.join(base_path, 'data', 's1_simple_dataset.pt')
    if not os.path.exists(data_path):
        print('Generating Data and Saving')
        starting_positions = np.linspace(0, 2 * np.pi, n_samples, endpoint=False)
        true_trajectories = np.ndarray((n_samples, trajectory_length))
        measurements = np.ndarray((n_samples, trajectory_length))

        for i in range(n_samples):
            # Generate a circular trajectory with a random starting position.
            initial_angle = starting_positions[i]
            trajectory = initial_angle + np.arange(trajectory_length) * step
            true_trajectories[i] = trajectory

            # Add Gaussian noise to the measurements.
            measurements[i] = (trajectory + np.random.normal(0, measurement_noise, trajectory_length)) % (2 * np.pi)

        measurements_ = torch.from_numpy(measurements % (2 * np.pi))[:, :, None].type(torch.FloatTensor)
        ground_truth_ = torch.from_numpy(true_trajectories % (2 * np.pi))[:, :, None].type(torch.FloatTensor)
        # ground_truth_flatten = torch.flatten(ground_truth_)[:, None].type(torch.FloatTensor)
        # measurements_flatten = torch.flatten(measurements_)[:, None].type(torch.FloatTensor)
        train_dataset = torch.utils.data.TensorDataset(ground_truth_, measurements_)
        torch.save(train_dataset, data_path)
    else:
        print('Loading Data')
        train_dataset = torch.load(data_path)

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, drop_last=True,
                                               shuffle=shuffle_flag)
    return train_loader


def total_loss(posterior, measurement_likelihood, value, band_limit, lambda_w):
    posterior_nll = loss_fn(posterior, value, band_limit)
    measurement_likelihood_nll = loss_fn(measurement_likelihood, value, band_limit)
    return lambda_w * posterior_nll + (1 - lambda_w) * measurement_likelihood_nll


def loss_fn(energy, value, grid_size):
    """Computes the loss function for the circular motion model.

    This function calculates the loss based on the negative log likelihood of the predicted distribution with the ground truth noisy measurements value
    Args:
      mu: The mean angle of the predicted distribution.
      cov: The covariance of the predicted distribution.
      measurements: The observed measurements.
      grid_size: The number of grid points to use for calculating the energy.

    Returns:
      ln_z_: The computed loss value.

    """

    eta = torch.fft.fftshift(torch.fft.fft(energy, dim=-1), dim=-1)
    maximum = torch.max(energy, dim=-1).values.unsqueeze(-1)
    moments = torch.fft.fft(torch.exp(energy - maximum), dim=-1)
    ln_z_ = torch.log(2 * math.pi * moments[..., 0] / grid_size).real.unsqueeze(-1) + maximum

    # taking inverse FFT over a set of frequencies ranging
    k_values = torch.arange(grid_size) - grid_size / 2
    k_values = k_values.unsqueeze(-1).unsqueeze(0)  # [ 1, num_samples, 1]

    # print(k_values.shape)
    value = value.unsqueeze(-1)  # [batch_size, 1,1]
    # print(value.shape)
    exponential_term = torch.exp(1j * k_values * value)  # [batch_size, num_samples, 1]
    # print(exponential_term.size())
    inverse_transform = (eta.unsqueeze(-1) * exponential_term).sum(dim=1).real  # [batch_size, 1]
    # print(inverse_transform.shape)
    return torch.mean(-inverse_transform / grid_size + ln_z_)


def unnormalized_von_mises_log_prob(samples, mu, kappa):
    """
    Computes the unnormalized log probability of a von Mises distribution.

    Args:
        samples: The input angle (in radians).  Shape: [batch_size, 1]
        mu: The mean angle of the von Mises distribution (in radians). Shape: [batch_size, 1]
        kappa: The concentration parameter of the von Mises distribution. Shape: [batch_size, 1]

    Returns:
        The unnormalized log probability of the von Mises distribution.
    """

    # Ensure that all inputs are PyTorch tensors.
    if not isinstance(samples, torch.Tensor):
        samples = torch.tensor(samples, dtype=torch.float32)
    if not isinstance(mu, torch.Tensor):
        mu = torch.tensor(mu, dtype=torch.float32)
    if not isinstance(kappa, torch.Tensor):
        kappa = torch.tensor(kappa, dtype=torch.float32)

    # Compute the unnormalized log probability.
    log_prob = kappa * torch.cos(samples - mu)

    return log_prob


def analytical_energy(mu, cov, band_limit):
    samples = torch.linspace(0.0, 2 * torch.pi, band_limit + 1)[:-1]
    samples = samples[None, :]  # Add a new dimension at the beginning
    samples_ = samples.repeat(band_limit, 1)
    kappa = np.reciprocal(cov)
    process_energy = unnormalized_von_mises_log_prob(samples, mu, kappa)
    return process_energy


class MeasurementModel(nn.Module):
    def __init__(self, input_size1, input_size2, hidden_size, output_size):
        super(MeasurementModel, self).__init__()
        self.fc1_1 = nn.Linear(input_size1, hidden_size)
        self.fc1_2 = nn.Linear(input_size2, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(2 * hidden_size, output_size)
        self.tanh = nn.Tanh()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x1, x2):
        out1 = self.fc1_1(x1)
        out2 = self.fc1_2(x2)
        out = self.sigmoid(torch.cat((out1, out2), dim=1))
        out = self.fc2(out)
        return out + x1


def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)


class HEFilter(nn.Module):
    def __init__(self, band_limit):
        super(HEFilter, self).__init__()
        self.band_limit = band_limit

    def predict(self, prior, process):
        ln_z_1 = self.normalisation_constant(prior)
        ln_z_2 = self.normalisation_constant(process)
        prob_1 = torch.exp(prior - ln_z_1)
        prob_2 = torch.exp(process - ln_z_2)

        moments_1 = torch.fft.fft(prob_1, dim=-1)
        moments_2 = torch.fft.fft(prob_2, dim=-1)
        moments_convolve = moments_1 * moments_2

        eta, energy = self.convert_moments_eta_energy(moments_convolve)
        return eta, energy

    def convert_moments_eta_energy(self, moments):
        prob = torch.fft.ifft(moments, dim=-1)
        ln_z_ = torch.unsqueeze(torch.real(torch.log(2 * math.pi * moments[:, 0] / self.band_limit)), 1)
        prob_real = torch.real(prob)
        prob_process = torch.where(prob_real > 0, prob_real, 1e-8)
        energy = torch.log(prob_process)
        eta = torch.fft.fft(energy, dim=-1)
        return eta, energy

    def convert_from_energy_eta(self, energy_samples):
        ln_z_ = self.normalisation_constant(energy_samples)
        eta = torch.fft.fft(energy_samples - ln_z_)
        return eta

    def convert_from_eta_energy(self, eta):
        energy = torch.fft.ifft(eta)
        prob_unnorm = torch.exp(energy)
        moments = torch.fft.fft(prob_unnorm)
        ln_z_ = torch.unsqueeze(torch.real(torch.log(2 * math.pi * moments[:, 0] / self.band_limit)), 1)
        energy_norm = torch.real(energy) - ln_z_
        return energy_norm

    def update(self, predict, measurements):
        eta = predict + self.convert_from_energy_eta(measurements)
        return self.convert_from_eta_energy(eta)

    def normalisation_constant(self, energy):
        maximum = torch.max(energy, dim=-1).values.unsqueeze(-1)
        moments = torch.fft.fft(torch.exp(energy - maximum))
        ln_z_ = torch.unsqueeze(torch.real(torch.log(2 * math.pi * moments[:, 0] / self.band_limit)), 1) + maximum
        return ln_z_


def main(batch_size, band_limit, hidden_size, learning_rate, num_epochs, noise_z, noise_p, initial_cov, step_size,
         traj_len, n_trajs, input_cov_z, initial_lambda, decay_rate, decay_lambda):
    measurement_model = MeasurementModel(band_limit, band_limit, hidden_size, band_limit)
    measurement_model.apply(init_weights)
    hef = HEFilter(band_limit)

    optimizer = optim.Adam(measurement_model.parameters(), lr=learning_rate)

    train_loader = generating_data_S1(batch_size, n_trajs, traj_len, noise_z, step_size, True)
    test_loader = generating_data_S1(batch_size, 20, traj_len, noise_z, step_size, False)

    c_time = time.strftime("%Y_%m_%d_%H_%M_%S")
    folder_path = f"{base_path}/results/s1_filter/{c_time}"
    os.makedirs(folder_path, exist_ok=True)
    for epoch in range(num_epochs):
        print(f"Training at epoch {epoch}")
        loss_tot = 0
        nll_posterior_tot = 0
        nll_likelihood_tot = 0
        epoch_folder_path = os.path.join(folder_path, f"epoch_{epoch}")

        if decay_lambda:
            current_lambda = initial_lambda * (decay_rate ** epoch)
        else:
            current_lambda = initial_lambda
        for i, (measurements, ground_truth) in enumerate(train_loader):
            prior = analytical_energy(ground_truth[:, 0], initial_cov, band_limit)
            control = torch.ones((batch_size, 1)) * step_size + torch.normal(0, noise_p,
                                                                             size=(batch_size, 1))  # Fixed Control
            for j in range(traj_len):
                energy_true_z = analytical_energy(measurements[:, j], input_cov_z, band_limit)
                energy_process = analytical_energy(control, noise_p ** 2, band_limit)

                # Predict Step
                eta_bel_x_t_bar, energy_bel_x_t_bar = hef.predict(prior, energy_process)

                # P(z_t | x_t) Measurement Step
                energy_pred_z = measurement_model(energy_true_z, energy_bel_x_t_bar)

                # Calculate Posterior
                with torch.no_grad():
                    energy_posterior = hef.update(eta_bel_x_t_bar, energy_pred_z)

                # Current posterior is prior for the next step (j + 1)
                prior = energy_posterior

                # Calculate loss
                loss = total_loss(energy_posterior, energy_pred_z, ground_truth[:, j], band_limit, current_lambda)
                nll_likelihood = loss_fn(energy_pred_z, ground_truth[:, j], band_limit)
                nll_posterior = loss_fn(energy_posterior, ground_truth[:, j], band_limit)

                # Update Measurement Model
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                loss_tot += loss.item()
                nll_posterior_tot += nll_posterior.item()
                nll_likelihood_tot += nll_likelihood.item()

                if epoch % 10 == 0 and i == 0:
                    os.makedirs(epoch_folder_path, exist_ok=True)
                    if j % 3 == 0:
                        plot_distributions(energy_posterior, energy_pred_z, energy_bel_x_t_bar, ground_truth,
                                           measurements, 0, j, epoch, noise_z, noise_p, band_limit, epoch_folder_path)

        if os.path.exists(epoch_folder_path):
            print(f"Generating gif for epoch {epoch}")
            video_path = f"{folder_path}/epoch_{epoch}.gif"
            generate_gif(epoch_folder_path, video_path)
            wandb.log({rf"{type}_{epoch}": wandb.Video(video_path, format="gif", )})

        wandb.log({
            'Epoch': epoch + 1,
            'loss': loss_tot / len(train_loader),
            'NLL Posterior': nll_posterior_tot / len(train_loader),
            'NLL Likelihood': nll_likelihood_tot / len(train_loader),
        })


def plot_distributions(energy_posterior, energy_pred_z, energy_bel_x_t_bar, ground_truth, measurements, i, j, epoch,
                       noise_z, noise_p, band_limit, folder_path, ax=None):
    print(f"Plotting Epoch {epoch} Traj{i} Iteration {j}")
    if ax is None:
        fig, ax = plt.subplots()
    energy_posterior = energy_posterior[i].detach()
    energy_pred_z = energy_pred_z[i].detach()
    energy_bel_x_t_bar = energy_bel_x_t_bar[i].detach()

    ax = plot_s1_energy([energy_posterior, energy_pred_z, energy_bel_x_t_bar],
                        ["posterior distribution", "measurement distribution", "predict distribution"], ax)
    ax = plotting_von_mises(ground_truth[i, j], noise_z ** 2, band_limit, ax, "true measurement distribution")
    ax = plotting_von_mises(ground_truth[i, j], noise_p ** 2, band_limit, ax, "true predict distribution")
    ax.plot(torch.cos(measurements[i, j]), torch.sin(measurements[i, j]), 'o', label="measurement data")
    ax.plot(torch.cos(ground_truth[i, j]), torch.sin(ground_truth[i, j]), 'o', label="pose data")
    ax.set_title(f"Epoch {epoch} Traj{i} Iteration {j}", loc='center')
    plt.savefig(f"{folder_path}/iter_{j}.png", format='png', dpi=300)
    ax.legend(bbox_to_anchor=(0.85, 1), loc='upper left', fontsize='x-small')
    plt.close()
    return ax


import argparse


def parse_args():
    parser = argparse.ArgumentParser(description='Train MLP Measurement Model with Analytical HEF PM on S1')
    parser.add_argument('--batch_size', type=int, default=45, help='Batch size for training')
    parser.add_argument('--n_trajs', type=int, default=100, help='Number of trajectories')
    parser.add_argument('--traj_len', type=int, default=30, help='Length of each trajectory')
    parser.add_argument('--noise_z', type=float, default=0.3, help='Measurement noise (Gaussian)')
    parser.add_argument('--noise_p', type=float, default=0.1, help='Process noise')
    parser.add_argument('--initial_cov', type=float, default=0.01, help='Initial covariance')
    parser.add_argument('--band_limit', type=int, default=60, help='Band limit')
    parser.add_argument('--step_size', type=float, default=0.1, help='Step size')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate')
    parser.add_argument('--num_epochs', type=int, default=1000, help='Number of epochs')
    parser.add_argument('--input_cov_z', type=float, default=0.025, help='Input covariance for Z')
    parser.add_argument('--input_cov', type=float, default=0.1, help='Input covariance for prior')
    parser.add_argument('--initial_lambda', type=float, default=0.7, help='Initial lambda for decay loss')
    parser.add_argument('--decay_rate', type=float, default=0.5, help='Decay rate for lambda decay loss')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--decay_lambda', type=int, choices=[0, 1], default=0,
                        help='flag set to true do lambda decay for the loss')

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    torch.manual_seed(args.seed)

    # If you are using CUDA
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)

    wandb.init(project="Diff-HEF", entity="korra141",
               tags=["s1", "analytical_PM", "learning_MM"],
               name="s1_filter_with_learning_MM",
               config=args)
    main(args.batch_size, args.band_limit, 32, args.learning_rate, args.num_epochs, args.noise_z, args.noise_p, args.initial_cov,args.step_size,
         args.traj_len, args.n_trajs, args.input_cov_z, args.initial_lambda, args.decay_rate, args.decay_lambda)
