# -*- coding: utf-8 -*-
"""CNN -  Uncertainity Estimation in R2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rbH7bwyRnOvF2quoHLLUeK13avykBx5l
"""

import math
import pdb
def calculate_normalisation_const(density,grid_size,range_x_diff,range_y_diff):
#   min_values = torch.min(density,dim=-1,keepdim=True)[0]
#   min_values = torch.min(min_values, dim=1,keepdim=True)[0]
  moments = torch.fft.fft2(density)
  z = (moments[:,0,0] * ((range_x_diff * range_y_diff) / math.prod(grid_size))).unsqueeze(-1).unsqueeze(-1)
  return z.real


def loss_fn(density, value,range_x,range_y,grid_size,step_t):

  id_x = ((value[:,0] - range_x[0])/step_t[0]).to(torch.int).to(torch.float) # [batchsize]
  id_y = ((value[:,1] - range_y[0])/step_t[1]).to(torch.int).to(torch.float)
  range_x_diff = range_x[1] - range_x[0]
  range_y_diff = range_y[1] - range_y[0]

  energy = torch.log(density + 1e-40)
  z = calculate_normalisation_const(density,grid_size,range_x_diff,range_y_diff)
  energy = energy  + torch.log(z)
  # density_scale = density * 10
  eta = torch.fft.fft2(energy) #[batchsize,grid_size1, grid_size2]

  k_x = torch.arange(grid_size[0]) # [grid_size1]
  k_y = torch.arange(grid_size[1]) # [grid_size2]

  temp_x = id_x.unsqueeze(-1) * k_x.unsqueeze(0) #[batchsize,grid_size1]
  temp_y = id_y.unsqueeze(-1) * k_y.unsqueeze(0) #[batchsize,grid_size2]

  exp_x = torch.exp(2j * math.pi * temp_x/grid_size[0])
  exp_y = torch.exp(2j * math.pi * temp_y/grid_size[1])

  temp_eta_y = torch.bmm(eta,exp_y.unsqueeze(-1)).squeeze(-1)

  reconstructed = (torch.bmm(temp_eta_y.unsqueeze(1), exp_x.unsqueeze(-1)) / math.prod(grid_size)).real


  # z = (torch.sum(density,dim=(1,2))*((range_x_diff * range_y_diff)/math.prod(grid_size))).unsqueeze(-1).unsqueeze(-1)
  result = -reconstructed + torch.log(z)
  # print("Integrating the distribution",torch.mean(torch.sum(density/z,dim=(1,2))*step_t[0]*step_t[1]))
  return torch.mean(result.squeeze(-1))

def calculate_true_nll(mean,std,value):
    # Calculate the true unnormalized density for each ground truth point in the batch
    exponent =  -((value[:,0:1] - mean[:, 0:1])**2 / (2 * std**2)) - ((value[:,1:2] - mean[:, 1:2])**2 / (2 * std**2))
    normalization = torch.tensor(2*math.pi*(std**2))
    return torch.mean(-exponent + torch.log(normalization))

# pose_density_1 = calculate_gaussian_density_batch(gt,initial_noise**2,band_limit)
# pose_density_1_0 = calculate_gaussian_density_batch(gt_noisy[0:1],initial_noise**2,band_limit)
# i=2

# loss_1 = loss_fn(pose_density_1,measurements[0:5],range_x,range_y,band_limit,step_t)
gt,gt_pose_density,measurements = next(iter(train_loader))

loss_ = loss_fn(gt_pose_density,measurements,range_x,range_y,band_limit,step_t)

# true_nll = calculate_true_nll(ground_truth,measurement_noise,measurements)
proposed_nll_ = calculate_true_nll(gt,initial_noise,measurements)

# print(loss_1)
# print(loss_)
# print(proposed_nll_)

# print(abs(loss_1 - proposed_nll_))

torch.mean(abs(loss_- proposed_nll_))
# print(id)

proposed_nll_.shape

# @title
# mean = torch.tensor([0.0,0.0]).unsqueeze(0)
# cov = 0.01
# mean = torch.tensor([[-0.4184, -0.3384]])
mean = torch.tensor([[3.2303e-01,  3.0303e-03]])
# std = 0.1
# band_limit=[50,50]
value = torch.tensor([[ 0.1600,  0.1800]])


pose_density_1 = calculate_gaussian_density_batch(gt_noisy,initial_noise**2,band_limit)
# i=2
# [i:i+1]
loss = loss_fn(pose_density_1,measurements,range_x,range_y,band_limit,step_t)

# true_nll = calculate_true_nll(ground_truth,measurement_noise,measurements)
proposed_nll = calculate_true_nll(gt_noisy,initial_noise,measurements)



id = torch.where(abs(loss-proposed_nll)>10)[0]
print(proposed_nll[id])
print(loss[id])

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import math
import numpy as np
import pdb

import numpy as np
import matplotlib.pyplot as plt
import random

def generate_fixed_length_trajectories(grid_start, grid_end, step, num_trajectories, trajectory_length,measurement_noise):
    # Define the grid points
    x_values = np.arange(grid_start, grid_end, step)
    y_values = np.arange(grid_start, grid_end, step)

    grid_points = np.array(np.meshgrid(x_values, y_values)).T.reshape(-1, 2)

    trajectories = []
    measurements = []

    for _ in range(num_trajectories):
        # Randomly select a starting point from the grid
        current_point = grid_points[np.random.choice(len(grid_points))]
        trajectory = []
        measurement = []

        for _ in range(trajectory_length):
            # Get possible moves
            moves = [
                [step, 0],  # Right
                [-step, 0], # Left
                [0, step],  # Up
                [0, -step],  # Down
                [step, step],  # Diagonal Right-Up
                [-step, step], # Diagonal Left-Up
                [step, -step], # Diagonal Right-Down
                [-step, -step] # Diagonal Left-Down
            ]

            valid_move_found = False
            while not valid_move_found:
                move = moves[np.random.choice(len(moves))]
                next_point = current_point + move
                noisy_point = ((next_point + np.random.normal(loc=0, scale=measurement_noise,size=(2)))//step)*step

                # Check if the next point stays within bounds
                if grid_start <= next_point[0] < grid_end and grid_start <= next_point[1] < grid_end:
                  if grid_start <= noisy_point[0] < grid_end and grid_start <= noisy_point[1] < grid_end:
                    valid_move_found = True
                    current_point = next_point
                    measurement_point = noisy_point

            trajectory.append(current_point)
            measurement.append(measurement_point)

        trajectories.append(np.array(trajectory))
        measurements.append(np.array(measurement))


    return np.array(trajectories), np.array(measurements)

# Parameters
# grid_start = -0.5
# grid_end = 0.5
# step = 0.02
# num_trajectories = 10
# trajectory_length = 10

# Generate trajectories
# trajectories,measurements = generate_fixed_length_trajectories(grid_start, grid_end, step, num_trajectories, trajectory_length,measurement_noise)

n_samples = 100
trajectory_length = 100
measurement_noise = 0.3
batch_size = 50
step_t=[0.02,0.02]
initial_noise = 0.1
band_limit=(50,50)
range_x = (-0.5,0.5)
range_y = (-0.5,0.5)
grid_start = -0.5
grid_end = 0.5
step = 0.02
range_x_diff = range_x[1] - range_x[0]
range_y_diff = range_y[1] - range_y[0]

true_trajectories, true_measurements = generate_fixed_length_trajectories(grid_start, grid_end, step, n_samples, trajectory_length,measurement_noise)
measurements_torch = torch.from_numpy(true_measurements).type(torch.FloatTensor)
ground_truth_torch = torch.from_numpy(true_trajectories).type(torch.FloatTensor)
ground_truth_flatten = torch.flatten(ground_truth_torch,start_dim=0,end_dim=1).type(torch.FloatTensor)
measurements_flatten = torch.flatten(measurements_torch,start_dim=0,end_dim=1).type(torch.FloatTensor)

trajectories = true_trajectories[0:10,0:10]
measurements_1 = true_measurements[0:10,0:10]
plt.figure(figsize=(6, 6))
# i ?= 0
for i, (trajectory,measurement1)  in enumerate(zip(trajectories,measurements_1)):
    plt.plot(trajectory[:, 0], trajectory[:, 1], label=f'Trajectory {i+1}')
    plt.scatter(trajectory[:, 0], trajectory[:, 1], s=10)  # Mark points
    plt.plot(measurement1 [:, 0], measurement1 [:, 1], label=f'Measurement {i+1}')
    plt.scatter(measurement1 [:, 0], measurement1 [:, 1], s=10)  # Mark points

num_divisions = 50
x_ticks = np.linspace(grid_start, grid_end, num_divisions+1)
y_ticks = np.linspace(grid_start, grid_end, num_divisions+1)
plt.xticks(x_ticks, rotation=90, fontsize=8)
plt.yticks(y_ticks, fontsize=8)
plt.grid(color='gray', linestyle='--', linewidth=0.5)

plt.title("Random Trajectories in R2 with 50x50 Grid")
plt.xlabel("x")
plt.ylabel("y")
plt.axis('equal')  # Equal aspect ratio
# plt.legend()
# plt.grid()
plt.show()

def generating_data_R2(n_samples, trajectory_length, measurement_noise, step_t,range_x,range_y):

  interval_length_x = range_x[1] - range_x[0]
  interval_length_y = range_y[1]- range_y[0]


  # uniformly sampling the starting position
  # starting_position_x = np.linspace(range_x[0],range_x[1], n_samples)
  # starting_position_y = np.linspace(range_y[0],range_y[1], n_samples)
  starting_position = np.random.uniform(-0.5, 0.5,size=(n_samples,2))

  true_trajectories = np.ndarray((n_samples, trajectory_length,2))
  measurements = np.ndarray((n_samples, trajectory_length,2))

  for i in range(n_samples):
    position = starting_position[i]
    for j in range(trajectory_length):
      position[0] = range_x[0] + (position[0] - range_x[0]) % interval_length_x
      position[1] = range_y[0] + (position[1] - range_y[0]) % interval_length_y
      true_trajectories[i,j] = position
      # temp_measurements = position + np.random.multivariate_normal([0,0],np.eye(2)*(measurement_noise**2),1)[0]
      temp_measurements = position + np.random.normal(loc=0, scale=measurement_noise,size=(2))

      # Make sure the measurements lie in the range as well on the grid.
      temp_measurements[0] = (range_x[0] + (temp_measurements[0] - range_x[0]) % interval_length_x)//step_t[0] * step_t[0]
      temp_measurements[1] = (range_y[0]+ (temp_measurements[1] - range_y[0]) % interval_length_y)//step_t[1] * step_t[1]
      measurements[i,j] = temp_measurements
      position = position + step_t

  return true_trajectories, measurements

pose_energy.shape

# ground_truth_noisy = ground_truth_flatten.clone()
# ground_truth_noisy[:,0] = range_x[0] + (ground_truth_flatten[:,0] + ((torch.randn_like(ground_truth_flatten[:,0]) * initial_noise)//step_t[0])*step_t[0] - range_x[0]) % range_x_diff
# ground_truth_noisy[:,1] = range_y[0] + (ground_truth_flatten[:,1] + ((torch.randn_like(ground_truth_flatten[:,1]) * initial_noise)//step_t[1])*step_t[1] - range_y[0]) % range_y_diff

pose_energy = calculate_gaussian_density_batch(ground_truth_flatten,initial_noise**2,band_limit)
train_dataset = torch.utils.data.TensorDataset(ground_truth_flatten, pose_energy, measurements_flatten)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)

import torch
import torch.nn as nn
import torch.nn.functional as F

class DensityPredictorCNN(nn.Module):
    def __init__(self, input_dim, grid_size,batch_size):
        """
        Args:
            input_dim: Number of input features.
            grid_size: Tuple (H, W) representing the dimensions of the grid.
        """
        super(DensityPredictorCNN, self).__init__()
        self.grid_size = grid_size

        self.input_padding = nn.ReplicationPad2d(1)

        # Convolutional layers for refinement
        self.conv1 = nn.Conv2d(1, 1, kernel_size=3,padding=1)
        self.conv2 = nn.Conv2d(1, 1, kernel_size=3,padding=1)
        self.conv3 = nn.Conv2d(1, 1, kernel_size=3,padding=1)
        self.batch_size = batch_size

    def forward(self, input):

        # Convolutional layers

        x = input.unsqueeze(1)
        x = self.input_padding(x)
        x = F.leaky_relu(self.conv1(x))
        x = F.leaky_relu(self.conv2(x))
        x = self.conv3(x)
        x = F.relu(x.squeeze(1))
        # batch_size, dim1, dim2, *rest = x.shape
        # x = x.view(batch_size, dim1 * dim2, *rest)

        # # # Apply softmax along the combined dimension (dim1 * dim2)
        # x = F.softmax(x, dim=1)

        # # # Reshape back to the original shape
        # x = x.view(batch_size, dim1, dim2, *rest)

        x = x[:,1:1+self.grid_size[0], 1:1+self.grid_size[1]]

        return torch.clamp(x,min=1e-10)

def init_weights(model):
  for layer in model.modules():
    initialize_conv_to_identity(layer)


def initialize_conv_to_zero(conv_layer):
    """
    Initialize a convolutional layer such that its output is close to zero.
    Args:
        conv_layer (nn.Conv2d): The convolutional layer to initialize.
    """
    if isinstance(conv_layer, nn.Conv2d):
      # Small random initialization for weights
      # nn.init.normal_(conv_layer.weight, mean=0.0, std=1e-3)
      nn.init.kaiming_uniform_(conv_layer.weight)
      nn.init.constant_(conv_layer.bias, 0.0)

def initialize_identity_with_torch(conv_layer):
    """
    Initialize a Conv2d layer as an identity transformation using PyTorch utilities.
    """
    if isinstance(conv_layer, nn.Conv2d):
      with torch.no_grad():
          # Get kernel size and ensure it's odd
          kernel_size = conv_layer.kernel_size[0]
          assert kernel_size % 2 == 1, "Kernel size must be odd for identity initialization."

          # Get the number of input and output channels
          out_channels, in_channels, h, w = conv_layer.weight.shape

          # Create an identity kernel
          identity_kernel = torch.zeros_like(conv_layer.weight)
          for i in range(min(out_channels, in_channels)):
              identity_kernel[i, i, kernel_size // 2, kernel_size // 2] = 1.0

          # Assign the identity kernel to the weights
          conv_layer.weight.copy_(identity_kernel)

          # Initialize bias to zero
          if conv_layer.bias is not None:
              nn.init.zeros_(conv_layer.bias)

def initialize_conv_to_identity(conv_layer):
    """
    Initialize a Conv2D layer to act as an identity mapping (mirroring the input).
    Args:
        conv_layer (nn.Conv2d): The Conv2D layer to initialize.
    """
    if isinstance(conv_layer, nn.Conv2d):
      with torch.no_grad():
        if conv_layer.weight.size(2) % 2 == 0 or conv_layer.weight.size(3) % 2 == 0:
            raise ValueError("Kernel size must be odd to achieve perfect identity mapping.")

        wts = torch.zeros(1, 1, 3, 3)
        nn.init.dirac_(wts)
        conv_layer.weight.copy_(wts)

        # Set biases to zero
        if conv_layer.bias is not None:
            nn.init.constant_(conv_layer.bias, 0.0)

# @title
def pad_for_fft_2d(tensor, target_shape):
        pad_h = target_shape[0] - tensor.shape[1]
        pad_w = target_shape[1] - tensor.shape[2]
        # Padding format in PyTorch: (left, right, top, bottom)
        # padded_tensor = torch.nn.functional.pad(tensor, (math.ceil(pad_w/2), pad_w - math.ceil(pad_w/2),math.ceil(pad_h/2), pad_h - math.ceil(pad_h/2)), mode='constant', value=0)
        padded_tensor = torch.nn.functional.pad(tensor, (0, pad_w, 0, pad_h), mode='constant', value=0)
        return padded_tensor
def convolve(prob_1, prob_2,band_limit):
        padded_length = (2*band_limit[0] - 1,2*band_limit[1] - 1)
        prob_1 = pad_for_fft_2d(prob_1, padded_length)
        prob_2 = pad_for_fft_2d(prob_2, padded_length)
        moments_1 = torch.fft.fft2(prob_1)
        moments_2 = torch.fft.fft2(prob_2)
        moments_convolve = moments_1 * moments_2
        unnorm_density_convolve = torch.fft.ifft2(moments_convolve)
        unnorm_density_convolve_final = unnorm_density_convolve[:,:band_limit[0] ,:band_limit[1]].real
        unnorm_density_convolve_final = torch.clamp(unnorm_density_convolve_final,min=1e-10)
        z_3 = normalisation_constant(unnorm_density_convolve_final,band_limit)
        density_convolve = unnorm_density_convolve_final/z_3
        return density_convolve
def normalisation_constant(density,band_limit):
        # maximum = torch.max(energy, dim=-1).values.unsqueeze(-1)
      moments = torch.fft.fft2(density)
      z_ = torch.real(moments[:, 0,0]/math.prod(band_limit)).unsqueeze(-1).unsqueeze(-1)
      return z_

def check_model_weights_nan(model):
    for name, param in model.named_parameters():
        if torch.isnan(param).any():
            print(f"NaN detected in model weights: {name}")

def get_gradients(model):
    gradients = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            gradients.append(param.grad.norm().item())
    return gradients

# prompt: Write a function to check if the tensor has nan values and print statements

def check_tensor_nan(tensor):
    if torch.isnan(tensor).any():
        print("Tensor contains NaN values.")

def histogram_density(measurements_2d, pose_2d, normalised_density, legend, ax):
  # Create a 2D histogram of the output energy distribution
  im = ax.imshow(normalised_density.detach().numpy().T, cmap='viridis', origin='lower', extent=[-0.5, 0.5, -0.5, 0.5])
  ax.set_title(legend)
  ax.scatter(measurements_2d[0].item(), measurements_2d[1].item(), color='red', label='Measurement', s=50)
  ax.scatter(pose_2d[0].item(), pose_2d[1].item(), color='blue', label='Pose', s=50)
  ax.set_xlabel('x')
  ax.set_ylabel('y')
  ax.legend()


  plt.colorbar(im, ax=ax, label='Density')

def plotting_3d_density(x,y,predicted_density,true_density,folder_path):
  fig, axs = plt.subplots(2, figsize=(8, 8),subplot_kw={'projection': '3d'})

  temp = axs[0].contour3D(x, y, predicted_density.numpy(), 50, cmap='viridis')
  temp_1 = axs[1].contour3D(x, y, true_density.numpy(), 50, cmap='viridis')

  # Add labels and title
  axs[0].set_xlabel('x')
  axs[0].set_ylabel('y')
  axs[0].set_zlabel('Predicted Measurement distribution')

  axs[1].set_xlabel('x')
  axs[1].set_ylabel('y')
  axs[1].set_zlabel('True Measurement distribution')

  # Add a color bar to show the values of z
  fig.colorbar(temp, ax=axs[0], shrink=0.5, aspect=5)
  fig.colorbar(temp_1, ax=axs[0], shrink=0.5, aspect=5)
  plt.savefig(folder_path + f"/plot3d.png")
  plt.close()

def plot_3d(ground_truth,measurement,band_limit, true_density, output_density, input_density ,folder_path):
  x = torch.linspace(-0.5, 0.5, band_limit[0]+1)[:-1]
  y = torch.linspace(-0.5, 0.5, band_limit[1]+1)[:-1]
  x, y = torch.meshgrid(x, y)
  x_numpy = x.numpy()
  y_numpy = y.numpy()

  output_density = output_density.detach()

  plotting_3d_density(x_numpy,y_numpy,output_density,true_density,folder_path)
  fig, axs = plt.subplots(2,2, figsize=(16, 16))

  # histogram_density(measurement, ground_truth, predicted_density, "predicted measurement likelihood", axs[0,0])
  histogram_density(measurement, ground_truth, output_density, "output NN", axs[0,1])
  histogram_density(measurement, ground_truth, input_density, "input_density_gt", axs[1,0])
  histogram_density(measurement, ground_truth, true_density, "true_density", axs[1,1])

  plt.tight_layout()
  plt.savefig(folder_path + f"/2d_plots.png")
  plt.close()

def kl_divergence(p, q):
    """
    Calculate the KL divergence between two distributions p and q.
    Both p and q should be torch tensors of the same shape.
    """
    epsilon = 1e-10  # Small value to avoid division by zero
    p = torch.clamp(p, min=epsilon)
    q = torch.clamp(q, min=epsilon)
    return abs(torch.mean(torch.sum(p * torch.log(p / q), dim=(1, 2))))

from google.colab import drive
drive.mount('/content/drive')

def calculate_gaussian_density_batch(mean,cov,band_limit):
    n_traj,dim = mean.shape
    # Create a meshgrid for the x and y coordinates
    x = torch.linspace(-0.5, 0.5, band_limit[0]+1)[:-1]
    y = torch.linspace(-0.5, 0.5, band_limit[1]+1)[:-1]
    x, y = torch.meshgrid(x, y)

    # Convert to numpy arrays for compatibility (if needed)
    x = torch.tile(x.unsqueeze(0),(n_traj,1,1))  # Add batch dimension
    y = torch.tile(y.unsqueeze(0),(n_traj,1,1))

    # Calculate the true unnormalized density for each ground truth point in the batch
    exponent =  -((x - mean[:, 0:1, None])**2 / (2 * cov)) - ((y - mean[:, 1:2, None])**2 / (2 * cov))
    normalization = torch.tensor(2*math.pi*(cov))
    return torch.exp(exponent)/normalization

# prompt: Write a training loop that call the meausrement model with flattened input of ground truth and learn the loss function on measurements as my target

import torch
import torch.optim as optim
import os
import datetime

# Assuming you have defined NeuralNetwork, loss_fn, train_loader, etc. as in your provided code.

# Define hyperparameters
learning_rate = 0.0001
num_epochs = 50
sample_idx = 0

# Initialize the model and optimizer
model = DensityPredictorCNN(np.prod(band_limit),band_limit,batch_size) # Assuming input and output dimensions are 3
model.apply(init_weights)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

loss_list = []
true_nll_list = []
kl_list = []

# Creating folders to log
run_name = "Uncertainity Estimation in R2"

current_datetime = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
log_dir = os.path.join("/content/drive/MyDrive","logs", run_name, current_datetime)
os.makedirs(log_dir, exist_ok=True)

lambda_ = 0.8

loss_batch = []

# Training loop
for epoch in range(num_epochs):
  loss_tot = 0
  kl_1_tot = 0
  kl_2_tot = 0
  true_nll_tot = 0
  proposed_nll_tot = 0
  hef_loss_temp = 0
  initial_nll_temp = 0
  for batch_idx, (ground_truth, ground_truth_energy, measurements) in enumerate(train_loader):

    check_model_weights_nan(model)
    outputs = model(ground_truth_energy)
    check_tensor_nan(outputs)

    z = calculate_normalisation_const(outputs,band_limit,range_x_diff,range_y_diff)
    check_tensor_nan(z)
    outputs = outputs/z

    hef_loss_input = loss_fn(ground_truth_energy,measurements,range_x,range_y,band_limit,step_t)

    initial_nll = calculate_true_nll(ground_truth,initial_noise,measurements)

    diff = abs(hef_loss_input - initial_nll)

    if diff > 0.1:
      print(f" for batch {batch_idx} of epoch {epoch}, checking loss batch function {diff}")

    loss = loss_fn(outputs, measurements,range_x,range_y,band_limit,step_t)
    true_nll_input = calculate_true_nll(ground_truth,measurement_noise,measurements)

    true_density = calculate_gaussian_density_batch(ground_truth, measurement_noise**2,band_limit)
    kl_div_2 = kl_divergence(true_density,outputs)
    # kl_div_1 = kl_divergence(outputs,true_density)


    optimizer.zero_grad()
    loss.backward()
    get_gradients(model)
    optimizer.step()

    loss_tot +=loss.item()
    kl_2_tot += kl_div_2.item()
    true_nll_tot += true_nll_input.item()
    initial_nll_temp += initial_nll.item()
    hef_loss_temp +=hef_loss_input.item()
    # proposed_nll_tot += proposed_nll.item()

    loss_batch.append(loss.item())
    # Print progress
    # print(f"Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}")
    if batch_idx==0 and (epoch) % 10 == 0:
      epoch_dir = log_dir + f"/epoch_{epoch}"
      os.makedirs(epoch_dir, exist_ok=True)
      plot_3d(ground_truth[sample_idx],measurements[sample_idx],band_limit,true_density[sample_idx],outputs[sample_idx],ground_truth_energy[sample_idx],epoch_dir)
    # Print training progress (optional)
  print(f"Target NLL Input : {initial_nll_temp/len(train_loader)}, Calculated NLL Input IUR: {hef_loss_temp/len(train_loader)}")
  print(f"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss_tot/(trajectory_length*len(train_loader)):.4f},KL :{kl_2_tot/(trajectory_length*len(train_loader)):.4f}")
  print(f"True NLL: {true_nll_tot/(trajectory_length*len(train_loader)):.4f}")
  loss_list.append(loss_tot/len(train_loader))
  true_nll_list.append(true_nll_tot/len(train_loader))
  kl_list.append(kl_2_tot/len(train_loader))

fig,ax = plt.subplots(2)
ax[0].plot(range(num_epochs),loss_list,label='loss_nll_measurement')
ax[0].plot(range(num_epochs),true_nll_list,label='true_nll')
# ax[0].plot(range(num_epochs),kl_list,label='kl_divergence')
ax[0].set_xlabel('Epoch')
ax[0].set_ylabel('Loss')
ax[0].legend()

ax[1].plot(range(len(loss_batch)),loss_batch)
ax[1].set_xlabel('Batch')
ax[1].set_ylabel('Loss')
# plt.savefig(log_dir + f"/loss.png")
plt.show()
plt.close()

