# -*- coding: utf-8 -*-
"""Uncertainity Estimation in R2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IoIpCjSx0W44Ixk9KXufr6kqeqoIOJNg
"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import math
import numpy as np

def generating_data_R2(n_samples, trajectory_length, measurement_noise, step_t,range_x,range_y):

  interval_length_x = range_x[1] - range_x[0]
  interval_length_y = range_y[1]- range_y[0]


  # uniformly sampling the starting position
  starting_position_x = np.linspace(range_x[0],range_x[1], n_samples)
  starting_position_y = np.linspace(range_y[0],range_y[1], n_samples)

  true_trajectories = np.ndarray((n_samples, trajectory_length,2))
  measurements = np.ndarray((n_samples, trajectory_length,2))

  for i in range(n_samples):
    position = np.array([starting_position_x[i],starting_position_y[i]])
    for j in range(trajectory_length):
      position[0] = range_x[0] + (position[0] - range_x[0]) % interval_length_x
      position[1] = range_y[0] + (position[1] - range_y[0]) % interval_length_y
      true_trajectories[i,j] = position
      temp_measurements = position + np.random.multivariate_normal([0,0],np.eye(2)*(measurement_noise**2),1)[0]
      # Make sure the measurements lie in the range as well on the grid.
      temp_measurements[0] = (range_x[0] + (temp_measurements[0] - range_x[0]) % interval_length_x)//step_t[0] * step_t[0]
      temp_measurements[1] = (range_y[0]+ (temp_measurements[1] - range_y[0]) % interval_length_y)//step_t[1] * step_t[1]
      measurements[i,j] = temp_measurements
      position = position + step_t

  return true_trajectories, measurements

idx = torch.tensor([1,2,3])
idy = torch.tensor([1,2,3])

energy = torch.rand((3,4,4))

energy[torch.arange(3), idx, idy]

energy[1,2,2]

import math
import pdb
def calculate_normalisation_const(energy,grid_size,range_x_diff,range_y_diff):
  max_values = torch.min(energy,dim=-1,keepdim=True)[0]
  max_values = torch.min(max_values, dim=1,keepdim=True)[0]
  # print(max_values.shape)
  moments = torch.fft.fft2(torch.exp(energy-max_values))
  ln_z_ = torch.log(moments[:,0,0] * ((range_x_diff * range_y_diff) / math.prod(grid_size))).unsqueeze(-1).unsqueeze(-1) + max_values
  return ln_z_.real


def loss_fn(energy, value,range_x,range_y,grid_size,step_t):


  id_x = ((value[:,0] - range_x[0])/step_t[0]).to(torch.int) # [batchsize]
  id_y = ((value[:,1] - range_y[0])/step_t[1]).to(torch.int)

  # eta = torch.fft.fft2(energy) #[batchsize,grid_size1, grid_size2]

  # k_x = torch.arange(grid_size[0]) # [grid_size1]
  # k_y = torch.arange(grid_size[1]) # [grid_size2]

  # temp_x = id_x.unsqueeze(-1) * k_x.unsqueeze(0) #[batchsize,grid_size1]
  # temp_y = id_y.unsqueeze(-1) * k_y.unsqueeze(0) #[batchsize,grid_size2]

  # exp_x = torch.exp(2j * math.pi * temp_x/grid_size[0])
  # exp_y = torch.exp(2j * math.pi * temp_y/grid_size[1])

  # temp = torch.matmul(eta,exp_x.unsqueeze(-1)).squeeze(-1)

  # reconstructed = (torch.matmul(temp.unsqueeze(1), exp_y.unsqueeze(-1)) / math.prod(grid_size)).real


  # range_x_diff = range_x[1] - range_x[0]
  # range_y_diff = range_y[1] - range_y[0]

  # # ln_z = calculate_normalisation_const(energy,grid_size,range_x_diff,range_y_diff)



  # pdb.set_trace()

  # z = (torch.sum(energy,dim=(1,2))*((range_x_diff * range_y_diff)/math.prod(grid_size))).unsqueeze(-1).unsqueeze(-1)

  # print("checking loss function")
  check_tensor_nan(z)
  check_tensor_nan(reconstructed)
  # print("finished checking loss function")
  # print("Numerical log Z:", ln_z[:2])

  # print("Log of likelihood at value", reconstructed[:2])

  # print("log Z moments",calculate_normalisation_const(energy))

  # print("Integrating the distribution",torch.mean(torch.sum(torch.exp(energy)/torch.exp(ln_z),dim=(1,2))*step_t[0]*step_t[1]))



  # return  - torch.mean(torch.log(reconstructed/z))
  return - torch.mean(torch.log(energy[torch.arange(energy.shape[0]),id_x,id_y]))

predicted_density[11]

mean = torch.tensor([0.0, 0.0]).unsqueeze(0)
std = 0.1
band_limit = (50,50)
density = calculate_gaussian_energy(mean,std,band_limit)



plot_gaussian_energy(predicted_density[11].detach())

value = torch.tensor([-0.4, 0.4]).unsqueeze(0)

predicted_density[11][5][45]

loss_fn(predicted_density[11].unsqueeze(0),value,range_x,range_y,band_limit,step_t)

predicted_density[11][5][45]

grid_size = band_limit
range_x = (-0.5,0.5)
range_y = (-0.5,0.5)
step_t = [0.02,0.02]
energy = predicted_density[11].unsqueeze(0)

id_x = ((value[:,0] - range_x[0])/step_t[0]).to(torch.int).to(torch.float) # [batchsize]
id_y = ((value[:,1] - range_y[0])/step_t[1]).to(torch.int).to(torch.float)

eta = torch.fft.fft2(energy) #[batchsize,grid_size1, grid_size2]

k_x = torch.arange(grid_size[0]) # [grid_size1]
k_y = torch.arange(grid_size[1]) # [grid_size2]

temp_x = id_x.unsqueeze(-1) * k_x.unsqueeze(0) #[batchsize,grid_size1]
temp_y = id_y.unsqueeze(-1) * k_y.unsqueeze(0) #[batchsize,grid_size2]

exp_x = torch.exp(2j * math.pi * temp_x/grid_size[0])
exp_y = torch.exp(2j * math.pi * temp_y/grid_size[1])

temp = torch.matmul(eta,exp_x.unsqueeze(-1)).squeeze(-1)

reconstructed = (torch.matmul(temp.unsqueeze(1), exp_y.unsqueeze(-1)) / math.prod(grid_size)).real


range_x_diff = range_x[1] - range_x[0]
range_y_diff = range_y[1] - range_y[0]

# ln_z = calculate_normalisation_const(energy,grid_size,range_x_diff,range_y_diff)



# pdb.set_trace()

z = (torch.sum(energy,dim=(1,2))*((range_x_diff * range_y_diff)/math.prod(grid_size))).unsqueeze(-1).unsqueeze(-1)

id_y

n_samples = 100
trajectory_length = 100
measurement_noise = 0.2
batch_size = 50
step_t=[0.02,0.02]
initial_noise = 0.1
band_limit=(50,50)
range_x = (-0.5,0.5)
range_y = (-0.5,0.5)
range_x_diff = range_x[1] - range_x[0]
range_y_diff = range_y[1] - range_y[0]

true_trajectories, measurements = generating_data_R2(n_samples, trajectory_length, measurement_noise, step_t,range_x,range_y)
measurements_ = torch.from_numpy(measurements).type(torch.FloatTensor)
ground_truth_ = torch.from_numpy(true_trajectories).type(torch.FloatTensor)
ground_truth_flatten = torch.flatten(ground_truth_,start_dim=0,end_dim=1).type(torch.FloatTensor)
measurements_flatten = torch.flatten(measurements_,start_dim=0,end_dim=1).type(torch.FloatTensor)

def calculate_gaussian_energy(mean,std,band_limit):
    n_samples = mean.shape[0]
    # Create a meshgrid for the x and y coordinates
    x = torch.linspace(-0.5, 0.5, band_limit[0]+1)[:-1]
    y = torch.linspace(-0.5, 0.5, band_limit[1]+1)[:-1]
    x, y = torch.meshgrid(x, y)

    # Convert to numpy arrays for compatibility (if needed)
    x = torch.tile(x.unsqueeze(0),(n_samples,1,1))  # Add batch dimension
    y = torch.tile(y.unsqueeze(0),(n_samples,1,1))

    # Calculate the true unnormalized density for each ground truth point in the batch
    exponent =  -((x - mean[:, 0:1, None])**2 / (2 * std**2)) - ((y - mean[:, 1:2, None])**2 / (2 * std**2))
    normalization = torch.tensor(2*math.pi*(std**2))
    return torch.exp(exponent)/normalization

pose_energy = torch.clamp(calculate_gaussian_energy(ground_truth_flatten,initial_noise,band_limit),min=1e-8)

plot_gaussian_energy(pose_energy[0])

def plot_gaussian_energy(energy):
  fig, axs = plt.subplots(1, figsize=(10, 8),subplot_kw={'projection': '3d'})
  x = np.linspace(-0.5, 0.5, band_limit[0],endpoint=False)
  y = np.linspace(-0.5, 0.5, band_limit[1],endpoint=False)
  x, y = np.meshgrid(x, y)
  # density = torch.exp(energy) / torch.sum(torch.exp(energy))
  temp = axs.contour3D(x, y, energy.numpy(), 50, cmap='viridis')
  # temp_1 = axs[0,1].contour3D(x_numpy, y_numpy, true_density.numpy(), 50, cmap='viridis')

  # Add labels and title
  axs.set_xlabel('x')
  axs.set_ylabel('y')
  axs.set_zlabel('Predicted distribution')

  # axs[0,1].set_xlabel('x')
  # axs[0,1].set_ylabel('y')
  # axs[0,1].set_zlabel('True distribution')

  # histogram_density(measurement, ground_truth_2d[0],normalised_density[0],"temp",ax=ax)
  # histogram_density(measurement, ground_truth_2d[0],normalised_density[0],"temp",ax=ax)


  # Add a color bar to show the values of z
  fig.colorbar(temp, ax=axs, shrink=0.5, aspect=5)
  # fig.colorbar(temp_1, ax=axs[1], shrink=0.5, aspect=5)

  # Show the plot
  plt.show()

train_dataset = torch.utils.data.TensorDataset(ground_truth_flatten, pose_energy, measurements_flatten)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)

from posix import X_OK
# prompt: write a neural network class that takes in input as dimension 2 and outputs unnormalised log density over size (50,50), Use sigmoid for itermitant layers and relu for the last layer

import torch
import torch.nn as nn

class NeuralNetwork(nn.Module):
    def __init__(self, input_dim, grid_size,batch_size):
        super(NeuralNetwork, self).__init__()
        self.batch_size = batch_size
        self.band_limit_x, self.band_limit_y = grid_size
        output_dim = self.band_limit_x * self.band_limit_y
        self.layer1 = nn.Linear(input_dim, 1000)
        self.layer2 = nn.Linear(1000,2000)
        self.layer3 = nn.Linear(2000, output_dim)
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.ReLU()

    def forward(self, input_energy):
        input_energy_reshaped = input_energy.view(self.batch_size,-1)
        # print(input_energy_reshaped.shape)
        x = self.sigmoid(self.layer1(input_energy_reshaped))
        x = self.sigmoid(self.layer2(x))
        x = self.layer3(x)
        x = self.relu(x)
        x = x.view(-1, self.band_limit_x, self.band_limit_y)
        return torch.clamp(x,min=1e-8,max=50)

def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)

# @title
# # prompt: I want to create a dataset of trajectories in SE(2) space. Imagine a robot at a randomly initalized point in SE2 and then from their through a sequence of control get trajectories.

# import numpy as np

# def generating_data_SE2(n_samples, trajectory_length, measurement_noise, step_t=[0.1,0.2], step_r=0.1):
#   """Generates training data for a system with circular motion.

#   Args:
#     n_samples: The total number of samples to generate.
#     trajectory_length: The length of each trajectory.
#     measurement_noise: The standard deviation of the measurement noise.
#     step_t: The translation step in x and y direction
#     step_r: The rotation step

#   Returns:
#     true_trajectories: Ground truth trajectories.
#     measurements: Noisy measurements of the trajectories.
#   """

#   # Initialize arrays to store trajectories and measurements
#   true_trajectories = np.ndarray((n_samples, trajectory_length, 3))
#   measurements = np.ndarray((n_samples, trajectory_length, 3))

#   for i in range(n_samples):
#     # Randomly initialize the starting position
#     start_x = np.random.uniform(-1, 1)  # Example: Initialize x between -1 and 1
#     start_y = np.random.uniform(-1, 1)  # Example: Initialize y between -1 and 1
#     start_theta = np.random.uniform(0, 2 * np.pi)  # Initialize theta between 0 and 2*pi

#     position = SE2Group.from_parameters(start_x, start_y, start_theta)
#     # Define the constant step
#     step = SE2Group.from_parameters(step_t[0], step_t[1], step_r)

#     for j in range(trajectory_length):
#       true_trajectories[i, j] = position.parameters()
#       # Add noise to the measurements
#       measurements[i, j] = position.parameters() + np.random.randn(3) * measurement_noise
#       # Update the position
#       position = position @ step
#       # Keep the angle within 0 to 2*pi
#       position.parameters()[2] = position.parameters()[2] % (2 * np.pi)

#   return true_trajectories, measurements

def convert_energy_density(energy,grid_size,range_x_diff,range_y_diff):
  # ln_z_ = calculate_normalisation_const(energy,grid_size,range_x_diff,range_y_diff)
  ln_z_ = torch.log(torch.sum(torch.exp(energy),dim=(1,2))*((range_x_diff * range_y_diff)/math.prod(grid_size))).unsqueeze(-1).unsqueeze(-1)
  return torch.exp(energy - ln_z_)

def check_model_weights_nan(model):
    for name, param in model.named_parameters():
        if torch.isnan(param).any():
            print(f"NaN detected in model weights: {name}")

def get_gradients(model):
    gradients = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            gradients.append(param.grad.norm().item())
    return gradients

# prompt: Write a function to check if the tensor has nan values and print statements

def check_tensor_nan(tensor):
    if torch.isnan(tensor).any():
        print("Tensor contains NaN values.")

def convolution_distribution(distribution_1,distribution_2,range_x_diff ,range_y_diff,grid_size):
  unnorm_dist = torch.fft.ifft2(torch.fft.fft2(distribution_1) * torch.fft.fft2(distribution_2)).real
  moments = torch.fft.fft2(unnorm_dist)
  z = (moments[:,0,0].real * ((range_x_diff * range_y_diff) / math.prod(grid_size))).unsqueeze(-1).unsqueeze(-1)
  return unnorm_dist/z

# prompt: Write a training loop that call the meausrement model with flattened input of ground truth and learn the loss function on measurements as my target

import torch
import torch.optim as optim
import os
import datetime

# Assuming you have defined NeuralNetwork, loss_fn, train_loader, etc. as in your provided code.

# Define hyperparameters
learning_rate = 0.001
num_epochs = 1000
sample_idx = 0

# Initialize the model and optimizer
model = NeuralNetwork(np.prod(band_limit),band_limit,batch_size) # Assuming input and output dimensions are 3
model.apply(init_weights)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
loss_list = []

# Creating folders to log
run_name = "Uncertainity Estimation in R2"

current_datetime = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
log_dir = os.path.join("logs", run_name, current_datetime)
os.makedirs(log_dir, exist_ok=True)


# Training loop
for epoch in range(num_epochs):
  loss_tot = 0
  for batch_idx, (ground_truth, ground_truth_energy, measurements) in enumerate(train_loader):

    check_model_weights_nan(model)
    # print(batch_idx)
    outputs = model(ground_truth_energy)
    # print(torch.max(outputs))
    check_tensor_nan(outputs)
    predicted_density = convolution_distribution(ground_truth_energy,outputs,range_x_diff,range_y_diff,band_limit)
    loss = loss_fn(predicted_density, measurements ,range_x,range_y,band_limit,step_t)
    # print(loss)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_tot +=loss.item()

    if batch_idx==0 and epoch % 10 == 0:
      epoch_dir = log_dir + f"/epoch_{epoch}"
      os.makedirs(epoch_dir, exist_ok=True)
      plot_3d_surface(ground_truth[sample_idx],measurements[sample_idx],measurement_noise,band_limit,predicted_density[sample_idx],ground_truth_energy[sample_idx],epoch_dir)

    # Print training progress (optional)
  print(f"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss_tot/len(train_loader):.4f}")


  loss_list.append(loss_tot/len(train_loader))

fig,ax = plt.subplots()
ax.plot(range(len(num_epochs)),loss_list)
plt.show()

# @title
# # prompt: write a 2d normal distribution

# import torch
# import matplotlib.pyplot as plt

# # Assuming you have already defined the necessary variables like band_limit, etc.
# # from the provided code.  Replace with your actual values.
# band_limit = (20, 20)

# # Create a grid of x and y values
# x_values = torch.arange(band_limit[0])
# y_values = torch.arange(band_limit[1])
# x, y = torch.meshgrid(x_values, y_values)

# ground_2d[0]
# # Create a 2D normal distribution centered at (mean_x, mean_y)
# mean_x = ground_2d[0][]  # Replace with desired mean x
# mean_y = 10  # Replace with desired mean y
# sigma_x = 2 # Replace with desired standard deviation x
# sigma_y = 3 # Replace with desired standard deviation y


# # Calculate the probability density function (PDF) of the normal distribution
# # PDF = (1/(2*pi*sigma_x*sigma_y)) * exp(-((x-mean_x)^2/(2*sigma_x^2)) + ((y-mean_y)^2/(2*sigma_y^2)))

# # Use torch.exp for exponentiation
# z = torch.exp(-((x-mean_x)**2/(2*sigma_x**2)) - ((y-mean_y)**2/(2*sigma_y**2)))


# # Normalize the PDF so it integrates to 1
# z = z / torch.sum(z)


# # Plot the 2D normal distribution
# plt.imshow(z, extent=[x_values.min(), x_values.max(), y_values.min(), y_values.max()], origin='lower')
# plt.xlabel('x')
# plt.ylabel('y')
# plt.title('2D Normal Distribution')
# plt.colorbar(label='Probability Density')
# plt.show()

# Create a 3D figure


def plot_3d_surface(ground_truth,measurement,measurement_noise, band_limit,predicted_density,input_energy,folder_path):
  x = torch.linspace(-0.5, 0.5, band_limit[0]+1)[:-1]
  y = torch.linspace(-0.5, 0.5, band_limit[1]+1)[:-1]
  x, y = torch.meshgrid(x, y)
  x_numpy = x.numpy()
  y_numpy = y.numpy()

  predicted_density = predicted_density.detach()

  true_density = calculate_gaussian_energy(ground_truth.unsqueeze(0),measurement_noise,band_limit).squeeze(0)
  # true_density = torch.exp(true_energy) / torch.sum(torch.exp(true_energy))

  # input_energy = input_energy.detach()
  # input_energy = torch.exp(input_energy) / torch.sum(torch.exp(input_energy))

  plotting_3d_density(x_numpy,y_numpy,predicted_density,true_density,folder_path)
  histogram_density(measurement, ground_truth, predicted_density, "predicted_density",folder_path)
  histogram_density(measurement, ground_truth, input_energy, "input_energy",folder_path)
  histogram_density(measurement, ground_truth, true_density, "true_energy",folder_path)

def histogram_density(measurements_2d,pose_2d,normalised_density,legend,folder_path):
  # Create a 2D histogram of the output energy distribution
  plt.figure(figsize=(10, 8))
  plt.imshow(normalised_density.detach().numpy(), cmap='viridis', origin='lower', extent=[-0.5, 0.5, -0.5, 0.5])
  plt.colorbar(label='Density')
  plt.title(legend)

  # Plot the measurement point
  plt.scatter(measurements_2d[0].item(), measurements_2d[ 1].item(), color='red', label='Measurement', s=50)
  plt.scatter(pose_2d[0].item(), pose_2d[ 1].item(), color='blue', label='Pose', s=50)
  plt.xlabel('x')
  plt.ylabel('y')
  plt.legend()
  plt.savefig(folder_path + "/" + legend + ".png")

def plotting_3d_density(x,y,predicted_density,true_density,folder_path):
  fig, axs = plt.subplots(2, figsize=(10, 8),subplot_kw={'projection': '3d'})

  temp = axs[0].contour3D(x, y, predicted_density.numpy(), 50, cmap='viridis')
  temp_1 = axs[1].contour3D(x, y, true_density.numpy(), 50, cmap='viridis')

  # Add labels and title
  axs[0].set_xlabel('x')
  axs[0].set_ylabel('y')
  axs[0].set_zlabel('Predicted distribution')

  axs[1].set_xlabel('x')
  axs[1].set_ylabel('y')
  axs[1].set_zlabel('True distribution')

  # Add a color bar to show the values of z
  fig.colorbar(temp, ax=axs[0], shrink=0.5, aspect=5)
  fig.colorbar(temp_1, ax=axs[0], shrink=0.5, aspect=5)

  plt.savefig(folder_path + "/plot3d")

import torch
import numpy as np
import matplotlib.pyplot as plt

def multivariate_gaussian_2d(grid_x, grid_y, mean, covariance):
    """
    Calculate the values of a 2D multivariate Gaussian distribution on a grid.

    Args:
    - grid_x, grid_y: Meshgrid arrays of shape [N, N] representing the x and y coordinates.
    - mean: Tensor of shape [2] representing the mean of the distribution.
    - covariance: Tensor of shape [2, 2] representing the covariance matrix.

    Returns:
    - Tensor of shape [N, N] representing the Gaussian distribution values on the grid.
    """
    grid = torch.stack([grid_x, grid_y], dim=-1)
    diff = grid - mean
    inv_cov = torch.linalg.inv(covariance)
    exponent = -0.5 * torch.einsum('...i,ij,...j->...', diff, inv_cov, diff)
    return torch.exp(exponent)/(2*np.pi*torch.sqrt(torch.det(covariance)))

def unnormalize_gaussian_2d(gaussian, constant):
    """
    Unnormalize a given 2D Gaussian distribution by multiplying it with a constant.
    """
    return constant * gaussian

def estimate_normalization_constant(unnormalized_gaussian):
    """
    Estimate the normalization constant using FFT.
    """
    # Perform the 2D FFT
    fft_result = torch.fft.fft2(unnormalized_gaussian)

    # Extract the zero frequency component (DC component)
    dc_component = torch.real(fft_result[0, 0])

    # Normalize by the total number of elements in the grid
    normalization_constant = dc_component / (unnormalized_gaussian.shape[0] * unnormalized_gaussian.shape[1])
    return normalization_constant

# Step 1: Set up the grid
N = 100
x = torch.linspace(-5, 5, N)
y = torch.linspace(-5, 5, N)
grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')

# Step 2: Define mean and covariance for the 2D Gaussian
mean = torch.tensor([0.0, 0.0])
covariance = torch.tensor([[1.0, 0.5], [0.5, 1.0]])

# Step 3: Generate the Gaussian distribution on the grid
gaussian = multivariate_gaussian_2d(grid_x, grid_y, mean, covariance)

# Step 4: Unnormalize the Gaussian by multiplying with a constant
constant = 5.0
unnormalized_gaussian = unnormalize_gaussian_2d(gaussian, constant)

# Step 5: Estimate the normalization constant using FFT
estimated_constant = estimate_normalization_constant(unnormalized_gaussian)

print(f"Original constant: {constant}")
print(f"Estimated constant using FFT: {estimated_constant.item()}")

# Plot the unnormalized Gaussian distribution
plt.figure(figsize=(6, 6))
plt.contourf(grid_x.numpy(), grid_y.numpy(), unnormalized_gaussian.numpy(), levels=50, cmap='viridis')
plt.title("Unnormalized 2D Gaussian Distribution")
plt.colorbar()
plt.show()

